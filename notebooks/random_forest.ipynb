{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4664d535",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4852c4",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Motivation](#1)\n",
    "- [2 - Sampling with replacement](#2)\n",
    "- [3 - Random forest algorithm](#3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c680199c",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "# 1 - Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c9b9f",
   "metadata": {},
   "source": [
    "One of the weaknesses of using a single decision tree is that that decision tree can be highly sensitive to small changes in the data. One solution to make the algorrithm less sensitive or more robust is to build not one decision tree, but to build a lot of decision trees. And we call that a tree ensemble.\n",
    "\n",
    "The reason we use an ensemble of trees is by having lots of decision trees and having them vote, it makes your overall algorithm less sensitive to what any single tree may be doing because it gets only one vote out of many, many different votes and it make your overall algorithm more robust. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf104080",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "# 2 - Sampling with replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006b946",
   "metadata": {},
   "source": [
    "In order to build a tree ensemble, we're going to need a technique called sampling with replacement. This technique is used to construct multiple random training sets that are all slightly different from our original training set. The procedure is displayed in the picture below with tokens. If you sample four times with replacement out of this bag, it means that you pick out one token, put it back in, and then take on another one.\n",
    "\n",
    "\n",
    "<img src=\"images/sampling_with_replacement_example.jpg\" style=\"width:300;height:300px;\">\n",
    "<caption><center><font><b>Figure 3</b>: Sampling with replacement example</center></caption>\n",
    "    \n",
    "The way that sampling with replacement applies to building an ensemble of tree is as follow. Using this technique we create a new random training set of training examples of the exact same size as the original data set. It turns out that this would be the key building block for building an ensemble of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38caf364",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "# 3 - Random forest algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e03271",
   "metadata": {},
   "source": [
    "#### Bagged decision tree\n",
    "\n",
    "\n",
    "Given training set of size m \\\n",
    "For *b*=1 to *B*: \n",
    "\n",
    "    1. Use sampling with replacement to create a new training set of size *m* \\\n",
    "    2. Train a decision tree on the new dataset\n",
    "    \n",
    "    \n",
    "The key idea is that even with this sampling with replacement procedure sometimes you end up with always using the same split at the root node and very similar splits near the root node. So there's one modification to the algorithm to further try to randomize the feature choice at each node that can cause the set of trees you learn to become more different from each other. The way this is typically done is at every node when choosing a feature to split on rather than picking from all features, we will instead pick a random subset of features and allow the algorithm to choose only from that subset of features.\n",
    "\n",
    "One way to think about why this is more robust to a single decision tree is the sampling with replacement procedure causes the algorithm to explore a lot of small changes to the data already and it's training different decision trees and is averaging over all of those changes to the data.\n",
    "\n",
    "At each node, when choosing a feature to use to split, if *n* features are available, pick a random subset of *k* < *n* features and allow the algorithm to only choose from that subset of features. A typical value for k would be k = $\\sqrt{n}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44694b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
